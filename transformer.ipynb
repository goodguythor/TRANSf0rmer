{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46329f66",
   "metadata": {},
   "source": [
    "# Transformer \n",
    "Transformer from scratch with NumpPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0b42b",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e001b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if u haven't install the library yet\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b988ccf",
   "metadata": {},
   "source": [
    "## Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of sentences\n",
    "sentences = [\n",
    "    \"noodle is delicious\",\n",
    "    \"i like hot rice\",\n",
    "    \"the weather is cold\",\n",
    "    \"that rice is cold\",\n",
    "    \"i eat hot noodle\",\n",
    "    \"good weather today\",\n",
    "    \"noodle and rice are food\",\n",
    "    \"i eat food\",\n",
    "    \"the food is good\",\n",
    "    \"the noodle is good\"\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa50d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0838787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words from sentences\n",
    "words = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        words.add(word)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each words with unique id\n",
    "vocab = {\n",
    "    \"<S>\":0, # start of the sentence\n",
    "    \"<E>\":1, # end of the sentence\n",
    "    \"<P>\":2 # padding/empty word \n",
    "}\n",
    "\n",
    "# map each id to word\n",
    "rvocab = {\n",
    "    0:\"<S>\", # start of the sentence\n",
    "    1:\"<E>\", # end of the sentence\n",
    "    2:\"<P>\" # padding/empty word \n",
    "}\n",
    "\n",
    "for i, word in enumerate(words, start=3):\n",
    "    vocab[word] = i\n",
    "    rvocab[i] = word\n",
    "\n",
    "vocab # word to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvocab # id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fucntion to tokenize the sentence based on our vocab\n",
    "def tokenize(sentence, vocab, seq_len=5):\n",
    "    tokens = [vocab.get(word.lower()) for word in sentence.split()]\n",
    "    return [vocab[\"<S>\"]] + tokens + [vocab[\"<E>\"]] + [vocab[\"<P>\"]]*max(0, seq_len-len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each sentences\n",
    "tokenized_sentence = [tokenize(sentence,vocab) for sentence in sentences]\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding for the token\n",
    "embedding_dimension = 4\n",
    "embedding_matrix = np.random.rand(len(vocab), embedding_dimension) * 0.01\n",
    "embedding = embedding_matrix[tokenized_sentence]\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551723f",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the sinusoidal positional encoding\n",
    "def sinusoidalPositionalEncoding(seq_len, d, n=10000):\n",
    "    encoding = np.zeros((seq_len, d))\n",
    "    for pos in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            theta = pos/denominator\n",
    "            encoding[pos, 2*i] = np.sin(theta)\n",
    "            encoding[pos, 2*i+1] = np.cos(theta)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc89db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token length & embedding dimension\n",
    "token_len = len(tokenized_sentence[0])\n",
    "embedding_dimension = embedding.shape[-1]\n",
    "\n",
    "# create positional encoding based on token_len & embedding_dimension\n",
    "positional_enc = sinusoidalPositionalEncoding(token_len, embedding_dimension)\n",
    "\n",
    "# add positional encoding to embedding\n",
    "embedding_with_positional_enc = embedding + positional_enc\n",
    "embedding_with_positional_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_with_positional_enc.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d517e",
   "metadata": {},
   "source": [
    "## Causal Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11165711",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_len = embedding_with_positional_enc.shape[1]\n",
    "causal_mask = np.triu(np.ones((mask_len, mask_len)) * -1e9, k=1)\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de042caf",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ce110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x/np.sum(e_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b310f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test softmax with random input\n",
    "for i in range(10):\n",
    "    random_input = np.random.randn(5)\n",
    "    softmax_output = softmax(random_input, axis=-1)\n",
    "    # use np.isclose to handle floating point precision issues\n",
    "    assert np.isclose(softmax_output.sum(), 1.0), \"sum of all softmax output must equal to 1\"\n",
    "    assert softmax_output.min() >= 0 and softmax_output.max() <= 1, \"softmax output must be around 0<=x<=1\"\n",
    "\"Softmax test passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d52115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for scaled dot product attention\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None, multi=False):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, (K.transpose(0, 1, 3, 2) if multi is True else K.transpose(0, 2, 1))) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores+=mask\n",
    "    weights = softmax(scores)\n",
    "    output = np.matmul(weights, V)\n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa961806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test single head function \n",
    "W_Q = np.random.rand(4, 4)*0.01\n",
    "W_K = np.random.rand(4, 4)*0.01\n",
    "W_V = np.random.rand(4, 4)*0.01\n",
    "\n",
    "Q = np.matmul(embedding_with_positional_enc, W_Q)\n",
    "K = np.matmul(embedding_with_positional_enc, W_K)\n",
    "V = np.matmul(embedding_with_positional_enc, W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc28d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f033c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191754ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test without masking mask\n",
    "single_output, single_weights = scaled_dot_product_attention(Q, K, V)\n",
    "single_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d389fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with mask\n",
    "single_masked_output, single_masked_weights = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "single_masked_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7885860",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_idx = 1\n",
    "v_sentence = sentences[s_idx]\n",
    "# plot weight before being masked\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(single_weights[s_idx], cmap='inferno', aspect='auto', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.title(f\"Single Unmasked Attention Weights '{v_sentence}'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot weight after being masked\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(single_masked_weights[s_idx], cmap='inferno', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.title(f\"Single Masked Attention Weights '{v_sentence}'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_masked_output.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242da127",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_masked_weights.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04d9fd",
   "metadata": {},
   "source": [
    "## Multi-Head Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c09631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "    head_dim = d_model // num_heads\n",
    "    x = x.reshape(batch_size, seq_len, num_heads, head_dim)\n",
    "    return x.transpose(0, 2, 1, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    return x.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, num_heads * head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc96c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for multi head attention\n",
    "def multi_head_attention(X, num_heads):\n",
    "    assert num_heads>0, \"number of heads must be more than 0\"\n",
    "    \n",
    "    d_model = X.shape[-1]\n",
    "    assert d_model % num_heads == 0, \"input dimension must be divisible by number of heads\"\n",
    "\n",
    "    # init random Q, V, K\n",
    "    W_Q = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_K = np.random.randn(d_model, d_model) * 0.01\n",
    "    W_V = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "    Q = np.matmul(X, W_Q)  # [batch, seq_len, d_model]\n",
    "    K = np.matmul(X, W_K)\n",
    "    V = np.matmul(X, W_V)\n",
    "\n",
    "    # split into heads\n",
    "    Q = split_heads(Q, num_heads)  # [batch, heads, seq_len, head_dim]\n",
    "    K = split_heads(K, num_heads)\n",
    "    V = split_heads(V, num_heads)\n",
    "\n",
    "    # attention for each head\n",
    "    attention_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask=causal_mask, multi=True) # use causal mask & add multi=True to use the correct transpose function\n",
    "\n",
    "    # combine heads\n",
    "    output = combine_heads(attention_output)  \n",
    "\n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get multi head attention output & weights based on our embedding input\n",
    "heads = 2\n",
    "out, weights = multi_head_attention(embedding_with_positional_enc, heads)\n",
    "(out, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0848c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93301ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape # (batch, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9ef30",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff=16):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) * 0.01\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) * 0.01\n",
    "        self.b2 = np.zeros(d_model)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        hidden = np.maximum(0, x @ self.W1 + self.b1)  # ReLU\n",
    "        return hidden @ self.W2 + self.b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(embedding_dimension) # init the ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_out = ffn(out)\n",
    "ffn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147889a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_out.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529a48d",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44058f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, eps=1e-6):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292047b9",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual + LayerNorm after multi-head attention\n",
    "attn_residual = layer_norm(embedding_with_positional_enc + out)\n",
    "\n",
    "# Residual + LayerNorm after feed-forward\n",
    "ffn_residual = layer_norm(attn_residual + ffn_out)\n",
    "\n",
    "ffn_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3742065",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_residual.shape # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01a139",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e38c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose ffn_residual is your final transformer block output\n",
    "vocab_size = len(vocab)\n",
    "d_model = ffn_residual.shape[-1]\n",
    "W_out = np.random.randn(d_model, vocab_size) * 0.01\n",
    "b_out = np.zeros(vocab_size)\n",
    "\n",
    "# Project to vocabulary size (logits for each token)\n",
    "logits = ffn_residual @ W_out + b_out\n",
    "logits.shape  # (batch, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa440070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random sentence from sentences list\n",
    "sentence_idx = 2\n",
    "print(f\"The sentence is {sentences[sentence_idx]}\")\n",
    "sentence_token = tokenized_sentence[sentence_idx]\n",
    "\n",
    "# get last word in sentence (not <P>/<E>)\n",
    "idx = len(sentence_token)-1\n",
    "while(sentence_token[idx] <= 2):\n",
    "    idx-=1\n",
    "\n",
    "# get the sentence that we want to predict\n",
    "predicted_sentence = [rvocab[sentence_token[i]] for i in range(idx)]\n",
    "\n",
    "# based on the sentence the word should be this\n",
    "print(f\"Based on the sentence, the last word of sequence {predicted_sentence} should be {rvocab[sentence_token[idx]]}\")\n",
    "\n",
    "idx-=1 # do this to exclude the last word because we want to predict what the last word are\n",
    "\n",
    "# get the logits of the same sentence and exclude special words in vocab like (<S>, <P>, <E>)\n",
    "last_logits_token = logits[sentence_idx, idx][3:]\n",
    "\n",
    "# get the next token probability\n",
    "probs = softmax(last_logits_token)\n",
    "\n",
    "# get all word exceb special words\n",
    "word_list = [rvocab[i] for i in range(3, vocab_size)]\n",
    "\n",
    "# print the probability of each words\n",
    "for i in range(len(probs)):\n",
    "    print(f\"Word: '{word_list[i]}' have a probability of {probs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(word_list, probs)\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f\"Next Word Probability Distribution After '{predicted_sentence}'\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
